## 1 机器学习的一些概念
> 有监督/无监督、拟合/欠拟合、泛化、交叉验证   

根据训练数据是否拥有标记信息，学习任务可以大致分为两大类：“监督学习”（supervised learning）和“无监督学习”（unsupervised learning），分类和回归是前者的代表，聚类是后者的代表。  

**有监督学习：** 学习给定标签的数据集，多为分类和回归；     

**无监督学习：** 学习没有给定标签的数据集，多为聚类和预测；   

**泛化能力：** 学得模型适用于新样本的能力，称为“泛化”能力，具有泛化能力的模型能很好地适用于整个样本空间；     

**过拟合：** 当学习器把训练样本学得“太好”了的时候，很可能已经把训练样本自身的一些特点当作了所有潜在样本都会具有的一般性质性质，这样就会导致泛化性能下降。这种现象在机器学习中称为“过拟合。   

```
过拟合的原因有很多，主要是由模型复杂、维度过量、训练集较小等造成； 该部分没有一个套用的方法，应该根据模型做不断的尝试，努力找到一个各个指标都均衡的模型。

解决方法：

1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。

2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。

3）采用正则化方法。正则化方法包括L0正则、L1正则和L2正则，而正则一般是在目标函数之后加上对于的范数。但是在机器学习中一般使用L2正则，下面看具体的原因。

4）采用dropout方法。这个方法在神经网络里面很常用。dropout方法是ImageNet中提出的一种方法，通俗一点讲就是dropout方法在训练的时候让神经元以一定的概率不工作。
```

**欠拟合：** 与过拟合相对应，模型不能很好的拟合数据特征，主要是模型不够复杂或者模型无法准确捕捉数据特征； 

```
欠拟合比较容易克服，例如在决策树学习中扩展分支；在神经网络学习中增加训练次数。

解决方法：

1）添加其他特征项，有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。

2）添加多项式特征，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。例如上面的图片的例子。

3）减少正则化参数，正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。

过拟合无法彻底避免，只能“缓解”，或者减小风险。
```

**交叉验证：**  判断模型的好和坏，就是衡量模型的（方差+偏差）和的最小值。因此主要的关注点就是平衡Bias和Variance。模型的Error = Bias + Variance，Error反映的是整个模型的准确度，Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度，Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。

现在通用的衡量方法采用的是交叉验证的思想。交叉验证思想能够很好的处理方差大和偏差大这两大痛点，能够更好的评估模型好坏！	

交叉验证的目的：在实际训练中，模型通常对训练数据好，但是对训练数据之外的数据拟合程度差。用于评价模型的泛化能力，从而进行模型选择。

交叉验证的基本思想：把在某种意义下将原始数据(dataset)进行分组,一部分做为训练集(train set),另一部分做为验证集(validation set or test set),首先用训练集对模型进行训练,再利用验证集来测试模型的泛化误差。另外，现实中数据总是有限的，为了对数据形成重用，从而提出k-折叠交叉验证。

对于个分类或回归问题，假设可选的模型为。k-折叠交叉验证就是将训练集的1/k作为测试集，每个模型训练k次，测试k次，错误率为k次的平均，最终选择平均率最小的模型Mi。
		


## 2 线性回归原理
线性回归是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，运用十分广泛。其表达形式为y = w'x+e，e为误差，服从均值为0的正态分布。 现在主要的目的就是求得最佳系数w.

> 回归分析中，只包括一个自变量和一个因变量，且二者的关系可用一条直线近似表示，这种回归分析称为一元线性回归分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为多元线性回归分析。  
> **多元线性仍属于线性回归，许多一元线性的理论仍适用于多元线性；**   
> 若为非线性关系，模型的创建和优化更加复杂。  


## 3.线性回归的损失函数，代价函数，目标函数 
损失函数（Loss Function ）是定义在单个样本上的，算的是一个样本的误差。

代价函数（Cost Function ）是定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均。

目标函数（Object Function）定义为：最终需要优化的函数。等于经验风险+结构风险（也就是Cost Function + 正则化项）。

关于目标函数和代价函数的区别还有一种通俗的区别：目标函数是最大化或者最小化，而代价函数是最小化

## 4.优化方法： （梯度下降法，牛顿法，拟牛顿法）

**(1)梯度下降法：**   
    1）首先对θ赋值，这个值可以是随机的，也可以是一个零向量；   
	2）改变θ的值，使得J(θ)按梯度下降的方向进行减少；   
	3）当J(θ)下降到无法下降时为止，即J(θ)对θ的导数为0时，比较J(θ)的值是否有变化。
		
**(2)牛顿法：**
无约束条件的最优化问题，假设目标函数J(θ)，J(θ)具有二阶连续偏导数，若第k次迭代值为 θ(k)，则可将J(θ(k+1))在θ(k)附近进行二阶泰勒展开：   

- $ J(θ(k+1))=J(θ(k))+J′(θ(k))(θ(k+1)−θ(k))+12J′′(θ(k))(θ(k+1)−θ(k))2       $
- $ J(θ(k+1))=J(θ(k))+J′(θ(k))(θ(k+1)−θ(k))+12J″(θ(k))(θ(k+1)−θ(k))2 $    
如果 $ θ(k+1)θ(k+1)$ 趋近于$ θ(k)θ(k)$时，  
$limθ(k+1)−θ(k)→0J(θ(k+1))−J(θ(k))=0limθ(k+1)−θ(k)→0J(θ(k+1))−J(θ(k))=0$，  带入上式，可以得到更新参数集合θθ的迭代公式：
- $θ(k+1)=θ(k)−J′(θ(k))J′′(θ(k))   $   
- $θ(k+1)=θ(k)−J′(θ(k))J″(θ(k))  $   
其中，$J′′(θ(k))J″(θ(k))$为$J(θ(k))J(θ(k))$的海塞矩阵（Hesse Matrix），上式中$[J′′(θ(k))]−1[J″(θ(k))]−1$即为海塞矩阵的逆矩阵。

**(3)拟牛顿法：**  
由于牛顿法中海塞矩阵的逆矩阵计算相对复杂，拟牛顿法通过一个nn阶矩阵$G(θ(k))G(θ(k))$来近似代替$[J′′(θ(k))]−1[J″(θ(k))]−1$。 
		
牛顿法中，海塞矩阵需要满足条件：

- $J′(θ(k+1))−J′(θ(k))=J′′(θ(k))(θ(k+1)−θ(k)) $  
- $J′(θ(k+1))−J′(θ(k))=J″(θ(k))(θ(k+1)−θ(k)) $    
上式变形，得到拟牛顿条件：
$[J′′(θ(k))]−1(J′(θ(k+1))−J′(θ(k)))=θ(k+1)−θ(k)   $
$[J″(θ(k))]−1(J′(θ(k+1))−J′(θ(k)))=θ(k+1)−θ(k) $ 
拟牛顿法将$G(θ(k))G(θ(k))$作为$[J′′(θ(k))]−1[J″(θ(k))]−1$的近似，则$G(θ(k))G(θ(k))$需要满足如下条件：     
    - 1）每次迭代矩阵$G(θ(k))G(θ(k))$为正定矩阵；   
    - 2）$G(θ(k))G(θ(k))$满足拟牛顿条件，即 $G(θ(k))(J′(θ(k+1))−J′(θ(k)))=θ(k+1)−θ(k)G(θ(k))(J′(θ(k+1))−J′(θ(k)))=θ(k+1)−θ(k)$。     
> 该部分理论很重要，但作为使用者，知道原理即可，公式的推导暂缓吧。 

## 5.线性回归的评估指标    
- 残差估计     
    总体思想是计算实际值与预测值间的差值简称残差。从而实现对回归模型的评估，一般可以画出残差图，进行分析评估、估计模型的异常值、同时还可以检查模型是否是线性的、以及误差是否随机分布
- 均方误差(Mean Squared Error, MSE)   
    均方误差是线性模型拟合过程中，最小化误差平方和(SSE)代价函数的平均值。MSE可以用于不同模型的比较，或是通过网格搜索进行参数调优，以及交叉验证等。
- 决定系数   
	可以看做是MSE的标准化版本，用于更好地解释模型的性能。换句话说，决定系数是模型捕获相应反差的分数。		

## 6、sklearn参数详解
```python
LinearRegression(fit_intercept=True,normalize=False,copy_X=True,n_jobs=1)
```
**参数：**   

- fit_intercept:布尔型，默认为true     
  是否对训练数据进行中心化。如果该变量为false，则表明输入的数据已经进行了中心化，在下面的过程里不进行中心化处理；否则，对输入的训练数据进行中心化处理。

- normalize:布尔型，默认为false   
  是否对数据进行标准化处理。

- copy_X:布尔型，默认为true   
  是否对X复制，如果选择false，则直接对原数据进行覆盖。（即经过中心化，标准化后，是否把新数据覆盖到原数据上）。

- n_jobs:整型， 默认为1   
  计算时设置的任务个数(number of jobs)。如果选择-1则代表使用所有的CPU。这一参数的对于目标个数>1（n_targets>1）且足够大规模的问题有加速作用。

- coef_：数组型变量， 形状为(n_features,)或(n_targets, n_features)。     
  说明：对于线性回归问题计算得到的feature的系数。如果输入的是多目标问题，则返回一个二维数组(n_targets, n_features)；如果是单目标问题，返回一个一维数组(n_features,)。

- intercept_ ：数组型变量。   
  说明：线性模型中的独立项。

---